{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9067824,"sourceType":"datasetVersion","datasetId":5469176}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%pip install transformers datasets evaluate torch nltk rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:43:23.248359Z","iopub.execute_input":"2024-07-30T14:43:23.249091Z","iopub.status.idle":"2024-07-30T14:43:39.518075Z","shell.execute_reply.started":"2024-07-30T14:43:23.249056Z","shell.execute_reply":"2024-07-30T14:43:39.516925Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=55941017a9d440fdedd2e251f24e5521a4c715648cac8c41665c0063be47e863\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score, evaluate\nSuccessfully installed evaluate-0.4.2 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Directly imported Dataset From Huggingface","metadata":{}},{"cell_type":"code","source":"# from datasets import Dataset, DatasetDict\n# from datasets import load_dataset\n# dataset = load_dataset(\"toughdata/quora-question-answer-dataset\")\n# dataset = dataset[\"train\"].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:43:39.520205Z","iopub.execute_input":"2024-07-30T14:43:39.520516Z","iopub.status.idle":"2024-07-30T14:43:39.524701Z","shell.execute_reply.started":"2024-07-30T14:43:39.520481Z","shell.execute_reply":"2024-07-30T14:43:39.523880Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:43:39.525760Z","iopub.execute_input":"2024-07-30T14:43:39.526017Z","iopub.status.idle":"2024-07-30T14:43:52.326977Z","shell.execute_reply.started":"2024-07-30T14:43:39.525994Z","shell.execute_reply":"2024-07-30T14:43:52.325951Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.1 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessed Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load preprocessed datasets\ndf_test_preprocessed = pd.read_csv('/kaggle/input/quora-quad-preprocessed/preprocessed_test_dataset.csv')\ndf_train_preprocessed = pd.read_csv('/kaggle/input/quora-quad-preprocessed/preprocessed_train_dataset.csv')\n\nfrom datasets import Dataset, DatasetDict\n\ntrain_dataset = Dataset.from_pandas(df_train_preprocessed)\ntest_dataset = Dataset.from_pandas(df_test_preprocessed)\ndataset_preprocessed= DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\n\nprint(\"Datasets converted back to original format for model training.\")\nfrom datasets import Dataset, DatasetDict\n\n# Ensure 'question' and 'answer' columns are of type string\ndf_train_preprocessed['question'] = df_train_preprocessed['question'].astype(str)\ndf_train_preprocessed['answer'] = df_train_preprocessed['answer'].astype(str)\n\ndf_test_preprocessed['question'] = df_test_preprocessed['question'].astype(str)\ndf_test_preprocessed['answer'] = df_test_preprocessed['answer'].astype(str)\n\ntrain_dataset = Dataset.from_pandas(df_train_preprocessed)\ntest_dataset = Dataset.from_pandas(df_test_preprocessed)\n\ndataset_preprocessed = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\nprint(dataset_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:43:52.328425Z","iopub.execute_input":"2024-07-30T14:43:52.328766Z","iopub.status.idle":"2024-07-30T14:43:55.363863Z","shell.execute_reply.started":"2024-07-30T14:43:52.328734Z","shell.execute_reply":"2024-07-30T14:43:55.362906Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Datasets converted back to original format for model training.\nDatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 44258\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 11187\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom datasets import load_dataset\nimport evaluate\nimport numpy as np\nfrom transformers import BartTokenizer, DataCollatorForSeq2Seq\nfrom transformers import BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport torch\n\n# Ensure GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the tokenizer and model for BART\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n\n# Load the data collator for BART\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:43:55.365901Z","iopub.execute_input":"2024-07-30T14:43:55.366344Z","iopub.status.idle":"2024-07-30T14:44:20.793428Z","shell.execute_reply.started":"2024-07-30T14:43:55.366318Z","shell.execute_reply":"2024-07-30T14:44:20.792642Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-30 14:44:02.415318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 14:44:02.415433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 14:44:02.543707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb3c7d0adec42868d3e7522e56c3379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4956d051e2944d74875379c3f4a5d06e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67fe3848aaae4dc68bedac0bc07d702c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144064d82b7e4d7090807197aaa790f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb6fb630e7e1474d8a46fced9d66e9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9db16467f44677a74587a322cb81db"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\nimport numpy as np\nimport nltk\nimport evaluate\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Define fraction of the dataset to use\nfraction = 0.01  # Use 10% of the dataset\n\n# Create smaller subsets of the dataset\ntrain_size = int(len(dataset_preprocessed[\"train\"]) * fraction)\ntest_size = int(len(dataset_preprocessed[\"test\"]) * fraction)\n\n# Shuffle and select the subset\ntrain_subset = dataset_preprocessed[\"train\"].shuffle(seed=42).select(range(train_size))\ntest_subset = dataset_preprocessed[\"test\"].shuffle(seed=42).select(range(test_size))\n\nprefix = \"answer the question: \"\n\ndef preprocess_function(examples):\n    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n    inputs = [prefix + doc for doc in examples[\"question\"]]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n    \n    labels = tokenizer(text_target=examples[\"answer\"], max_length=512, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Load BART tokenizer and model\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n\n# Tokenize the subset datasets\ntokenized_train_dataset = train_subset.map(preprocess_function, batched=True)\ntokenized_test_dataset = test_subset.map(preprocess_function, batched=True)\n\n# Set up ROUGE, BLEU, Precision, Recall, and F1 score for evaluation\nnltk.download(\"punkt\", quiet=True)\nrouge_metric = evaluate.load(\"rouge\")\nbleu_metric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # Decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # ROUGE expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    # Compute ROUGE score\n    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    # Compute BLEU score\n    decoded_preds_for_bleu = [\" \".join(pred.split()) for pred in decoded_preds]\n    decoded_labels_for_bleu = [[\" \".join(label.split())] for label in decoded_labels]\n    bleu_result = bleu_metric.compute(predictions=decoded_preds_for_bleu, references=decoded_labels_for_bleu)\n\n    # Compute Precision, Recall, and F1 on sentence level\n    true_labels = [label.split() for label in decoded_labels]\n    pred_labels = [pred.split() for pred in decoded_preds]\n\n    # Flatten the lists\n    true_labels_flat = [token for sublist in true_labels for token in sublist]\n    pred_labels_flat = [token for sublist in pred_labels for token in sublist]\n    \n    # Ensure the lengths match by truncating to the minimum length\n    min_length = min(len(true_labels_flat), len(pred_labels_flat))\n    true_labels_flat = true_labels_flat[:min_length]\n    pred_labels_flat = pred_labels_flat[:min_length]\n\n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, pred_labels_flat, average='weighted', zero_division=0)\n\n    # Combine metrics\n    combined_result = {\n        **rouge_result,\n        \"bleu\": bleu_result[\"score\"],\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n    return combined_result\n\n# Set up training arguments with logging configuration\n# training_args = Seq2SeqTrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n#     logging_strategy=\"steps\",\n#     logging_steps=50,  # Adjust the logging frequency as needed\n#     learning_rate=3e-4,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=4,\n#     weight_decay=0.01,\n#     save_total_limit=3,\n#     num_train_epochs=2,\n#     predict_with_generate=True,\n#     push_to_hub=False,\n#     report_to=\"none\",  # You can also set this to \"tensorboard\" if using TensorBoard\n# )\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,  # Increase logging frequency if needed\n    learning_rate=5e-5,  # Adjust learning rate\n    per_device_train_batch_size=8,  # Adjust batch size\n    per_device_eval_batch_size=4,  # Adjust batch size\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=5,  # Increase number of epochs\n    predict_with_generate=True,\n    push_to_hub=False,\n    report_to=\"none\",  # Use \"tensorboard\" if needed\n)\n\n# Set up trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:44:20.794701Z","iopub.execute_input":"2024-07-30T14:44:20.795295Z","iopub.status.idle":"2024-07-30T14:48:59.837170Z","shell.execute_reply.started":"2024-07-30T14:44:20.795260Z","shell.execute_reply":"2024-07-30T14:48:59.836006Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31a842ef7dc4a849c493555bd44ddf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/111 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d9e8eaa8d64b83ae2e3eb5b4ae66c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05225efe056842c4a215b7795a3e48d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b67474b8d840d98e8d0d9bc8659f18"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [280/280 04:27, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Bleu</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>5.969712</td>\n      <td>0.088763</td>\n      <td>0.023697</td>\n      <td>0.082052</td>\n      <td>0.082058</td>\n      <td>0.003844</td>\n      <td>0.004353</td>\n      <td>0.001088</td>\n      <td>0.001741</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>6.519000</td>\n      <td>5.958683</td>\n      <td>0.066584</td>\n      <td>0.015152</td>\n      <td>0.061768</td>\n      <td>0.062606</td>\n      <td>0.002152</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.519000</td>\n      <td>5.867538</td>\n      <td>0.083761</td>\n      <td>0.021389</td>\n      <td>0.075105</td>\n      <td>0.075429</td>\n      <td>0.007906</td>\n      <td>0.001875</td>\n      <td>0.002000</td>\n      <td>0.001916</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.520200</td>\n      <td>5.874949</td>\n      <td>0.097157</td>\n      <td>0.020737</td>\n      <td>0.089584</td>\n      <td>0.090122</td>\n      <td>0.003980</td>\n      <td>0.004756</td>\n      <td>0.002217</td>\n      <td>0.002273</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>5.520200</td>\n      <td>5.873425</td>\n      <td>0.093770</td>\n      <td>0.019564</td>\n      <td>0.086462</td>\n      <td>0.086930</td>\n      <td>0.002340</td>\n      <td>0.000344</td>\n      <td>0.001171</td>\n      <td>0.000532</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=280, training_loss=5.758419908796038, metrics={'train_runtime': 268.8748, 'train_samples_per_second': 8.219, 'train_steps_per_second': 1.041, 'total_flos': 123389518282752.0, 'train_loss': 5.758419908796038, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model('./results/final_model')","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:27:43.872224Z","iopub.execute_input":"2024-07-30T16:27:43.872810Z","iopub.status.idle":"2024-07-30T16:27:44.198653Z","shell.execute_reply.started":"2024-07-30T16:27:43.872765Z","shell.execute_reply":"2024-07-30T16:27:44.197293Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/final_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]}]}