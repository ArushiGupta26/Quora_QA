{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9067824,"sourceType":"datasetVersion","datasetId":5469176},{"sourceId":190480058,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T17:00:20.598290Z","iopub.execute_input":"2024-07-30T17:00:20.599165Z","iopub.status.idle":"2024-07-30T17:00:20.612959Z","shell.execute_reply.started":"2024-07-30T17:00:20.599133Z","shell.execute_reply":"2024-07-30T17:00:20.612049Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/quora-quad-preprocessed/preprocessed_test_dataset.csv\n/kaggle/input/quora-quad-preprocessed/preprocessed_train_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install transformers datasets evaluate torch nltk rouge_score\n!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:46:40.036919Z","iopub.execute_input":"2024-07-30T16:46:40.037294Z","iopub.status.idle":"2024-07-30T16:47:06.858994Z","shell.execute_reply.started":"2024-07-30T16:46:40.037268Z","shell.execute_reply":"2024-07-30T16:47:06.857818Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6705e6b6e083f8a42903af14a693cba5c12bde04be2c7473687552808baaa029\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.1 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Original Dataset","metadata":{}},{"cell_type":"code","source":"# from datasets import Dataset, DatasetDict\n# from datasets import load_dataset\n# dataset = load_dataset(\"toughdata/quora-question-answer-dataset\")\n# dataset = dataset[\"train\"].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:47:06.860751Z","iopub.execute_input":"2024-07-30T16:47:06.861659Z","iopub.status.idle":"2024-07-30T16:47:06.865528Z","shell.execute_reply.started":"2024-07-30T16:47:06.861629Z","shell.execute_reply":"2024-07-30T16:47:06.864665Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessed Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load preprocessed datasets\ndf_test_preprocessed = pd.read_csv('/kaggle/input/quora-quad-preprocessed/preprocessed_test_dataset.csv')\ndf_train_preprocessed = pd.read_csv('/kaggle/input/quora-quad-preprocessed/preprocessed_train_dataset.csv')\n\nfrom datasets import Dataset, DatasetDict\n\ntrain_dataset = Dataset.from_pandas(df_train_preprocessed)\ntest_dataset = Dataset.from_pandas(df_test_preprocessed)\ndataset_preprocessed= DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\n\nprint(\"Datasets converted back to original format for model training.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:47:06.867837Z","iopub.execute_input":"2024-07-30T16:47:06.868437Z","iopub.status.idle":"2024-07-30T16:47:09.247058Z","shell.execute_reply.started":"2024-07-30T16:47:06.868406Z","shell.execute_reply":"2024-07-30T16:47:09.246110Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Datasets converted back to original format for model training.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Ensure 'question' and 'answer' columns are of type string\ndf_train_preprocessed['question'] = df_train_preprocessed['question'].astype(str)\ndf_train_preprocessed['answer'] = df_train_preprocessed['answer'].astype(str)\n\ndf_test_preprocessed['question'] = df_test_preprocessed['question'].astype(str)\ndf_test_preprocessed['answer'] = df_test_preprocessed['answer'].astype(str)\n\ntrain_dataset = Dataset.from_pandas(df_train_preprocessed)\ntest_dataset = Dataset.from_pandas(df_test_preprocessed)\n\ndataset_preprocessed = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\nprint(dataset_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T16:47:09.248167Z","iopub.execute_input":"2024-07-30T16:47:09.248619Z","iopub.status.idle":"2024-07-30T16:47:09.529475Z","shell.execute_reply.started":"2024-07-30T16:47:09.248592Z","shell.execute_reply":"2024-07-30T16:47:09.528541Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 44258\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 11187\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom datasets import load_dataset\nimport evaluate\nimport numpy as np\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Ensure GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the tokenizer and model for DistilGPT-2\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(device)\n\n# Load the dataset\n# dataset = load_dataset(\"quora\")\n\n# Define fraction of the dataset to use\nfraction = 0.0005  # Use 0.05% of the dataset\n\n# Create smaller subsets of the dataset\ntrain_size = int(len(dataset[\"train\"]) * fraction)\ntest_size = int(len(dataset[\"test\"]) * fraction)\n\n# Shuffle and select the subset\ntrain_subset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\ntest_subset = dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n\nprefix = \"answer the question: \"\n\ndef preprocess_function(examples):\n    \"\"\"Concatenate question and answer with a prefix, and tokenize\"\"\"\n    inputs = [prefix + doc for doc in examples[\"question\"]]\n    outputs = examples[\"answer\"]\n    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(outputs, max_length=64, truncation=True, padding=\"max_length\")\n\n    # Concatenate input and output for causal language modeling\n    model_inputs[\"input_ids\"] = [input_ids + label_ids for input_ids, label_ids in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"])]\n    model_inputs[\"attention_mask\"] = [attn_mask + [1] * len(label_ids) for attn_mask, label_ids in zip(model_inputs[\"attention_mask\"], labels[\"input_ids\"])]\n\n    # Add padding token where necessary\n    max_length = max(len(x) for x in model_inputs[\"input_ids\"])\n    model_inputs[\"input_ids\"] = [x + [tokenizer.pad_token_id] * (max_length - len(x)) for x in model_inputs[\"input_ids\"]]\n    model_inputs[\"attention_mask\"] = [x + [0] * (max_length - len(x)) for x in model_inputs[\"attention_mask\"]]\n\n    # Shift the labels so the decoder learns to predict the next token\n    model_inputs[\"labels\"] = [[-100] * len(input_ids) + label_ids for input_ids, label_ids in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"])]\n\n    return model_inputs\n\n# Tokenize the subset datasets\ntokenized_train_dataset = train_subset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\ntokenized_test_dataset = test_subset.map(preprocess_function, batched=True, remove_columns=[\"question\", \"answer\"])\n\n# Load the data collator for GPT-2\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Set up ROUGE, BLEU, Precision, Recall, and F1 score for evaluation\nnltk.download(\"punkt\", quiet=True)\nrouge_metric = evaluate.load(\"rouge\")\nbleu_metric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # Convert predictions and labels to lists and flatten\n    preds = preds.tolist()\n    labels = labels.tolist()\n\n    # Debugging information\n    print(f\"First 5 preds: {preds[:5]}\")\n    print(f\"First 5 labels: {labels[:5]}\")\n\n    # Flatten nested lists\n    preds = [item for sublist in preds for item in sublist if item is not None]\n    labels = [item for sublist in labels for item in sublist if item is not None]\n\n    # Decode preds and labels\n    try:\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    except TypeError as e:\n        print(f\"Error during decoding: {e}\")\n        print(f\"Preds: {preds}\")\n        print(f\"Labels: {labels}\")\n        raise\n\n    # ROUGE expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    # Compute ROUGE score\n    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n\n    # Compute BLEU score\n    decoded_preds_for_bleu = [\" \".join(pred.split()) for pred in decoded_preds]\n    decoded_labels_for_bleu = [[\" \".join(label.split())] for label in decoded_labels]\n    bleu_result = bleu_metric.compute(predictions=decoded_preds_for_bleu, references=decoded_labels_for_bleu)\n\n    # Compute Precision, Recall, and F1 on sentence level\n    true_labels = [label.split() for label in decoded_labels]\n    pred_labels = [pred.split() for pred in decoded_preds]\n\n    # Flatten the lists\n    true_labels_flat = [token for sublist in true_labels for token in sublist]\n    pred_labels_flat = [token for sublist in pred_labels for token in sublist]\n    \n    # Ensure the lengths match by truncating to the minimum length\n    min_length = min(len(true_labels_flat), len(pred_labels_flat))\n    true_labels_flat = true_labels_flat[:min_length]\n    pred_labels_flat = pred_labels_flat[:min_length]\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, pred_labels_flat, average='weighted', zero_division=0)\n\n    # Combine metrics\n    combined_result = {\n        **rouge_result,\n        \"bleu\": bleu_result[\"score\"],\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n    return combined_result\n\n# Set up training arguments with logging configuration\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=50,  # Increase or decrease as needed\n    learning_rate=5e-5,  # Adjust learning rate\n    per_device_train_batch_size=1,  # Further reduced batch size\n    per_device_eval_batch_size=1,  # Further reduced batch size\n    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n    fp16=True,  # Use mixed precision training\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,  # Reduce number of epochs\n    report_to=\"none\",  # Use \"tensorboard\" if needed\n)\n\n# Clear the CUDA cache\ntorch.cuda.empty_cache()\n\n# Set up trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('./results/final_model')","metadata":{},"execution_count":null,"outputs":[]}]}